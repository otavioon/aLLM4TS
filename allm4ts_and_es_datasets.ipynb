{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/home/vscode/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from minerva.models.ssl.tfc import TFC_Model\n",
    "from minerva.models.nets.tfc import TFC_Backbone\n",
    "from minerva.models.nets.tnc import TSEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils.timefeatures import time_features\n",
    "import warnings\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import random\n",
    "import tqdm\n",
    "\n",
    "\n",
    "from minerva.models.ssl.tfc import TFC_Model\n",
    "from minerva.models.nets.tfc import TFC_Backbone\n",
    "import warnings\n",
    "import warnings\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from minerva.data.datasets.series_dataset import MultiModalSeriesCSVDataset\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aLLM4TS Pretrain Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_pretrain(Dataset):\n",
    "    # X is the input data and Y is the input data shifted by stride (16)\n",
    "    def __init__(\n",
    "        self,\n",
    "        pt_data=\"ETTh1_ETTm1_ETTh2_ETTm2_weather_traffic_electricity_illness\",\n",
    "        patch_len=16,  # Not used\n",
    "        stride=16,  # Will be used as the shift (y = x + stride)\n",
    "        root_path=\"/workspaces/HIAAC-KR-Dev-Container/workspace/aLLM4TS/dataset/\",\n",
    "        flag=\"train\",\n",
    "        size=[1024, 0, 1024],  # [seq_len, label_len, pred_len]\n",
    "        features=\"M\",\n",
    "        data_path=\"ETTh1.csv\",  # Not used\n",
    "        target=\"OT\",\n",
    "        scale=True,\n",
    "        timeenc=1,\n",
    "        freq=\"h\",\n",
    "        percent=100,\n",
    "        return_values=\"x_y\",\n",
    "    ):\n",
    "        assert return_values in [\"x\", \"x_y\", \"x_y_mark\"]\n",
    "        self.return_values = return_values\n",
    "        self.pt_data = pt_data\n",
    "        self.patch_len = patch_len\n",
    "        self.stride = stride\n",
    "        if size == None:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            self.seq_len = size[0]\n",
    "            self.label_len = size[1]\n",
    "            self.pred_len = size[2]\n",
    "\n",
    "        assert flag in [\"train\", \"test\", \"val\"]\n",
    "        type_map = {\"train\": 0, \"val\": 1, \"test\": 2}\n",
    "        self.set_type = type_map[flag]  # 0: train, 1: val, 2: test\n",
    "\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.scale = scale\n",
    "        self.timeenc = timeenc\n",
    "        self.freq = freq\n",
    "        self.percent = percent\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.data_path = data_path\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        pt_datasets = self.pt_data.split(\"_\")\n",
    "\n",
    "        data_list = []\n",
    "        data_stamp_list = []\n",
    "        for pt_dataset in pt_datasets:\n",
    "            df_raw = pd.read_csv(\n",
    "                os.path.join(self.root_path, f\"{pt_dataset}.csv\")\n",
    "            )\n",
    "            dataset_len = len(df_raw)\n",
    "            # Handle datasets\n",
    "            if \"ETTh\" in pt_dataset:\n",
    "                border1s = [\n",
    "                    0,\n",
    "                    12 * 30 * 24 - self.seq_len,\n",
    "                    12 * 30 * 24 + 4 * 30 * 24 - self.seq_len,\n",
    "                ]\n",
    "                border2s = [\n",
    "                    12 * 30 * 24,\n",
    "                    12 * 30 * 24 + 4 * 30 * 24,\n",
    "                    12 * 30 * 24 + 8 * 30 * 24,\n",
    "                ]\n",
    "                border1 = border1s[self.set_type]\n",
    "                border2 = border2s[self.set_type]\n",
    "            elif \"ETTm\" in pt_dataset:\n",
    "                border1s = [\n",
    "                    0,\n",
    "                    12 * 30 * 24 * 4 - self.seq_len,\n",
    "                    12 * 30 * 24 * 4 + 4 * 30 * 24 * 4 - self.seq_len,\n",
    "                ]\n",
    "                border2s = [\n",
    "                    12 * 30 * 24 * 4,\n",
    "                    12 * 30 * 24 * 4 + 4 * 30 * 24 * 4,\n",
    "                    12 * 30 * 24 * 4 + 8 * 30 * 24 * 4,\n",
    "                ]\n",
    "                border1 = border1s[self.set_type]\n",
    "                border2 = border2s[self.set_type]\n",
    "            else:\n",
    "                num_train = int(dataset_len * 0.7)\n",
    "                num_test = int(dataset_len * 0.2)\n",
    "                num_vali = dataset_len - num_train - num_test\n",
    "                border1s = [\n",
    "                    0,\n",
    "                    num_train - self.seq_len,\n",
    "                    dataset_len - num_test - self.seq_len,\n",
    "                ]\n",
    "                border2s = [num_train, num_train + num_vali, dataset_len]\n",
    "                border1 = border1s[self.set_type]\n",
    "                border2 = border2s[self.set_type]\n",
    "\n",
    "            # Hendle some options\n",
    "            if self.set_type == 0:\n",
    "                border2 = (\n",
    "                    border2 - self.seq_len\n",
    "                ) * self.percent // 100 + self.seq_len\n",
    "            if self.features == \"M\" or self.features == \"MS\":\n",
    "                cols_data = df_raw.columns[1:]\n",
    "                df_data = df_raw[cols_data]\n",
    "            elif self.features == \"S\":\n",
    "                df_data = df_raw[[self.target]]\n",
    "\n",
    "            df_data = df_data.values\n",
    "\n",
    "            # Handle scaling. Scaling is done on each subset of the data, \n",
    "            # separately. Train data will be normalizedon train data, \n",
    "            # validation data will be normalized on validation data, etc.\n",
    "            if self.scale:\n",
    "                train_data = df_data[border1s[0] : border2s[0]]\n",
    "                self.scaler.fit(train_data)\n",
    "                data = self.scaler.transform(df_data)\n",
    "            else:\n",
    "                data = df_data\n",
    "\n",
    "            data = data[border1:border2]\n",
    "            data = data.reshape((len(data) * len(cols_data), 1))\n",
    "            df_stamp = df_raw[[\"date\"]][border1:border2]\n",
    "            df_stamp[\"date\"] = pd.to_datetime(df_stamp.date)\n",
    "            if self.timeenc == 0:\n",
    "                df_stamp[\"month\"] = df_stamp.date.apply(\n",
    "                    lambda row: row.month, 1\n",
    "                )\n",
    "                df_stamp[\"day\"] = df_stamp.date.apply(lambda row: row.day, 1)\n",
    "                df_stamp[\"weekday\"] = df_stamp.date.apply(\n",
    "                    lambda row: row.weekday(), 1\n",
    "                )\n",
    "                df_stamp[\"hour\"] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
    "                data_stamp = df_stamp.drop([\"date\"], axis=1).values\n",
    "            elif self.timeenc == 1:\n",
    "                data_stamp = time_features(\n",
    "                    pd.to_datetime(df_stamp[\"date\"].values), freq=self.freq\n",
    "                )\n",
    "                data_stamp = data_stamp.transpose(1, 0)\n",
    "\n",
    "            data_list.append(data)\n",
    "            df_stamp = np.array(\n",
    "                [data_stamp for i in range(len(cols_data))]\n",
    "            ).reshape((len(data_stamp) * len(cols_data), 4))\n",
    "            data_stamp_list.append(df_stamp)\n",
    "\n",
    "        self.data = np.concatenate(data_list, axis=0)\n",
    "        self.data_stamp = np.concatenate(data_stamp_list, axis=0)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_begin + self.stride\n",
    "        r_end = s_end + self.stride\n",
    "        # print(s_begin, s_end, r_begin, r_end)\n",
    "\n",
    "        # print(f\"Values: seq_len={self.seq_len}, patch_len={self.patch_len}, stride={self.stride}, s_begin={s_begin}, s_end={s_end}, r_begin={r_begin}, r_end={r_end}\")\n",
    "        # print(f\"Seq_x: from {s_begin} to {s_end}\")\n",
    "        # print(f\"Seq_y: from {r_begin} to {r_end}\")\n",
    "\n",
    "        seq_x = self.data[s_begin:s_end].swapaxes(0, 1)\n",
    "        seq_y = self.data[r_begin:r_end].swapaxes(0, 1)\n",
    "\n",
    "        if self.return_values == \"x\":\n",
    "            return seq_x\n",
    "        elif self.return_values == \"x_y\":\n",
    "            return seq_x, seq_y\n",
    "        else:\n",
    "            seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "            seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "            return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len - self.patch_len + 1\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 17878732 samples. x.shape=(1, 1024), y.shape=(1, 1024)\n",
      "Test passed! Y is same as X shifted by stride (16)\n"
     ]
    }
   ],
   "source": [
    "index = 100\n",
    "stride = 16\n",
    "\n",
    "pt_dset_train = Dataset_pretrain(stride=stride, return_values=\"x_y\", flag=\"train\")\n",
    "x, y = pt_dset_train[index]\n",
    "print(f\"Dataset has {len(pt_dset_train)} samples. x.shape={x.shape}, y.shape={y.shape}\")\n",
    "\n",
    "# Y is same as X shifted by stride (16)\n",
    "np.testing.assert_allclose(x[0, stride:1024], y[0, 0:1024-stride])\n",
    "print(f\"Test passed! Y is same as X shifted by stride ({stride})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 3932428 samples. x.shape=(1, 1024), y.shape=(1, 1024)\n",
      "Test passed! Y is same as X shifted by stride (16)\n"
     ]
    }
   ],
   "source": [
    "index = 100\n",
    "stride = 16\n",
    "\n",
    "pt_dset_val = Dataset_pretrain(stride=stride, return_values=\"x_y\", flag=\"val\")\n",
    "x, y = pt_dset_val[index]\n",
    "print(f\"Dataset has {len(pt_dset_val)} samples. x.shape={x.shape}, y.shape={y.shape}\")\n",
    "\n",
    "# Y is same as X shifted by stride (16)\n",
    "np.testing.assert_allclose(x[0, stride:1024], y[0, 0:1024-stride])\n",
    "print(f\"Test passed! Y is same as X shifted by stride ({stride})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 3932428 samples. x.shape=(1, 1024), y.shape=(1, 1024)\n",
      "Test passed! Y is same as X shifted by stride (16)\n"
     ]
    }
   ],
   "source": [
    "index = 100\n",
    "stride = 16\n",
    "\n",
    "pr_dset_test = Dataset_pretrain(stride=stride, return_values=\"x_y\", flag=\"val\")\n",
    "x, y = pr_dset_test[index]\n",
    "print(f\"Dataset has {len(pr_dset_test)} samples. x.shape={x.shape}, y.shape={y.shape}\")\n",
    "\n",
    "# Y is same as X shifted by stride (16)\n",
    "np.testing.assert_allclose(x[0, stride:1024], y[0, 0:1024-stride])\n",
    "print(f\"Test passed! Y is same as X shifted by stride ({stride})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExtraSensory Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base_Har_Dataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_path: str,\n",
    "        feature_prefixes=[\n",
    "            \"accel-x\",\n",
    "            \"accel-y\",\n",
    "            \"accel-z\",\n",
    "            \"gyro-x\",\n",
    "            \"gyro-y\",\n",
    "            \"gyro-z\",\n",
    "        ],\n",
    "        flag: str = \"train\",\n",
    "        size=[1024, 0, 1024],  # [seq_len, label_len, pred_len]\n",
    "        stride=16,\n",
    "        return_values=\"x_y\",\n",
    "        val_split: float = 0.1,\n",
    "        test_split: float = 0.2,\n",
    "        seed: int = 42,\n",
    "        scale=True,\n",
    "        percent: int = 100,\n",
    "        \n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        assert return_values in [\"x\", \"x_y\"]\n",
    "        assert flag in [\"train\", \"test\", \"val\"]\n",
    "\n",
    "        # From arguments\n",
    "        self.root_dir = Path(root_path)\n",
    "        self.feature_prefixes = feature_prefixes\n",
    "        self.flag = flag\n",
    "        self.seq_len, self.label_len, self.pred_len = size[0], size[1], size[2]\n",
    "        self.stride = stride\n",
    "        self.return_values = return_values\n",
    "        self.seed = seed\n",
    "        self.scale = scale\n",
    "        self.percent = percent\n",
    "\n",
    "        # Split the data\n",
    "        self.train_split = 1 - val_split - test_split\n",
    "        self.val_split = val_split\n",
    "        self.test_split = test_split\n",
    "        assert (\n",
    "            self.train_split + self.val_split + self.test_split == 1\n",
    "        ), \"Splits should sum to 1\"\n",
    "\n",
    "        # Read the data and set the data and x, y begin and end indices\n",
    "        self.scaler = StandardScaler()\n",
    "        self.data, self.effective_stride = self.__read_data__()\n",
    "\n",
    "    def _get_dataset_data(self) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __read_data__(self):\n",
    "        data = self._get_dataset_data()\n",
    "        original_shape = data.shape\n",
    "        effective_stride = self.stride * original_shape[1]\n",
    "        # Maintin only first dimension and flatten the rest\n",
    "        print(\n",
    "            f\"Data shape before flatten: {data.shape}. Effective stride: {effective_stride} ({self.stride}*{original_shape[1]})\"\n",
    "        )\n",
    "        # Flatten each sample in F order\n",
    "        data = np.ascontiguousarray(\n",
    "            data.reshape(original_shape[0], -1, order=\"F\")\n",
    "        )\n",
    "        print(\n",
    "            f\"Data shape after flatten (F-order): {data.shape}. Min: {data.min()}, Max: {data.max()}\"\n",
    "        )\n",
    "\n",
    "        if self.scale:\n",
    "            shape = data.shape\n",
    "            data = self.scaler.fit_transform(data.reshape(-1, 1)).reshape(shape)\n",
    "            # Reshape, do the scaling and reshape back\n",
    "            print(\n",
    "                f\"Data shape after scaling: {data.shape}. Min: {data.min()}, Max: {data.max()}\"\n",
    "            )\n",
    "\n",
    "        if self.percent < 100:\n",
    "            # Take only the first self.percent samples\n",
    "            data = data[: int(data.shape[0] * self.percent / 100)]\n",
    "            print(f\"Data shape after taking {self.percent}%: {data.shape}\")\n",
    "\n",
    "        # Now, we have data of shape (num_samples, num_features * num_timesteps)\n",
    "        # Where the last dimension is the flattened version of the original data\n",
    "        # interleaved (F-order)\n",
    "        return (data, effective_stride)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        seq_x = data[: -self.effective_stride]\n",
    "        seq_y = data[self.effective_stride :]\n",
    "\n",
    "        # 1024-264 = 760 zeros at the end\n",
    "        pad_size = max(0, self.seq_len - len(seq_x))\n",
    "        seq_x = np.pad(seq_x, (0, pad_size), mode=\"constant\", constant_values=0)\n",
    "        seq_y = np.pad(seq_y, (0, pad_size), mode=\"constant\", constant_values=0)\n",
    "\n",
    "        # Add a channel dimension at the beginning\n",
    "        seq_x = seq_x.reshape(1, -1)\n",
    "        seq_y = seq_y.reshape(1, -1)\n",
    "\n",
    "        if self.return_values == \"x\":\n",
    "            return seq_x\n",
    "        return seq_x, seq_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def inverse_transform(self, data):\n",
    "        shape = data.shape\n",
    "        data = data.reshape(-1, 1)\n",
    "        data = self.scaler.inverse_transform(data)\n",
    "        return data.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_ExtraSensory(Base_Har_Dataset):\n",
    "    def _get_dataset_data(self) -> np.ndarray:\n",
    "        rng = random.Random(self.seed)\n",
    "        # List files and shuffle it using the seed\n",
    "        files = sorted(list(self.root_dir.glob(\"es_full.*.csv\")))\n",
    "        rng.shuffle(files)\n",
    "\n",
    "        # Split the files\n",
    "        num_train = int(len(files) * self.train_split)\n",
    "        num_val = int(len(files) * self.val_split)\n",
    "\n",
    "        if self.flag == \"train\":\n",
    "            # First num_train files\n",
    "            files = files[:num_train]\n",
    "        elif self.flag == \"val\":\n",
    "            # Files from num_train to num_train + num_val\n",
    "            files = files[num_train : num_train + num_val]\n",
    "        else:\n",
    "            # Files from num_train + num_val to the end (last num_test files)\n",
    "            files = files[num_train + num_val :]\n",
    "\n",
    "        datasets = []\n",
    "        tqdm_bar = tqdm.tqdm(enumerate(files), total=len(files), desc=\"Reading files...\")\n",
    "        for i, f in tqdm_bar:\n",
    "            tqdm_bar.set_description(f\"Reading {f.name}\")\n",
    "            dataset = MultiModalSeriesCSVDataset(\n",
    "                f, self.feature_prefixes, label=None, features_as_channels=True\n",
    "            )\n",
    "\n",
    "            # This should return a numpy array of shape (num_samples, num_features, num_timesteps)\n",
    "            # In this case, (num_samples, 6, 60)\n",
    "            data = dataset[:].astype(np.float32)\n",
    "            datasets.append(data)\n",
    "\n",
    "        # Data is an array of shape (num_samples, num_features, num_timesteps)\n",
    "        data = np.concatenate(datasets, axis=0)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading es_full.100000.csv: 100%|██████████| 25/25 [03:20<00:00,  8.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape before flatten: (5469725, 6, 60). Effective stride: 96 (16*6)\n",
      "Data shape after flatten (F-order): (5469725, 360). Min: -79.97591400146484, Max: 92.86629486083984\n",
      "Data shape after scaling: (5469725, 360). Min: -37.31132888793945, Max: 43.129364013671875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5469725"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_es_dataset = Dataset_ExtraSensory(\n",
    "    root_path=\"/workspaces/HIAAC-KR-Dev-Container/some_datasets/ES_Raw/\",\n",
    "    stride=stride,\n",
    "    return_values=\"x_y\",\n",
    "    flag=\"train\",\n",
    ")\n",
    "len(train_es_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 5469725 samples. x.shape=(1, 1024), y.shape=(1, 1024)\n"
     ]
    }
   ],
   "source": [
    "index = 100\n",
    "x, y = train_es_dataset[index]\n",
    "print(f\"Dataset has {len(train_es_dataset)} samples. x.shape={x.shape}, y.shape={y.shape}\")\n",
    "\n",
    "np.testing.assert_allclose(\n",
    "    x[0, train_es_dataset.effective_stride:360-train_es_dataset.effective_stride],\n",
    "    y[0, 0:360-(train_es_dataset.effective_stride)*2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading es_full.280000.csv: 100%|██████████| 3/3 [00:32<00:00, 10.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape before flatten: (449675, 6, 60). Effective stride: 96 (16*6)\n",
      "Data shape after flatten (F-order): (449675, 360). Min: -36.92852783203125, Max: 39.63801193237305\n",
      "Data shape after scaling: (449675, 360). Min: -21.122934341430664, Max: 22.436803817749023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "449675"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_es_dataset = Dataset_ExtraSensory(\n",
    "    root_path=\"/workspaces/HIAAC-KR-Dev-Container/some_datasets/ES_Raw/\",\n",
    "    stride=stride,\n",
    "    return_values=\"x_y\",\n",
    "    flag=\"val\",\n",
    ")\n",
    "len(val_es_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading es_full.150000.csv: 100%|██████████| 8/8 [02:40<00:00, 20.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape before flatten: (1527750, 6, 60). Effective stride: 96 (16*6)\n",
      "Data shape after flatten (F-order): (1527750, 360). Min: -52.94058609008789, Max: 98.81349182128906\n",
      "Data shape after scaling: (1527750, 360). Min: -26.968708038330078, Max: 50.25152587890625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1527750"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_es_dataset = Dataset_ExtraSensory(\n",
    "    root_path=\"/workspaces/HIAAC-KR-Dev-Container/some_datasets/ES_Raw/\",\n",
    "    stride=stride,\n",
    "    return_values=\"x_y\",\n",
    "    flag=\"test\",\n",
    ")\n",
    "len(test_es_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAGHAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_DAGHAR(Base_Har_Dataset):\n",
    "    def _get_dataset_data(self) -> np.ndarray:\n",
    "        if self.flag == \"train\":\n",
    "            files = self.root_dir.rglob(\"train.csv\")\n",
    "        elif self.flag == \"val\":\n",
    "            files = self.root_dir.rglob(\"validation.csv\")\n",
    "        else:\n",
    "            files = self.root_dir.rglob(\"test.csv\")\n",
    "        files = sorted(list(files))\n",
    "        print(f\"Selected {len(files)} files for {self.flag}: {files}\")\n",
    "\n",
    "        datasets = []\n",
    "        tqdm_bar = tqdm.tqdm(\n",
    "            enumerate(files), total=len(files), desc=\"Reading files...\"\n",
    "        )\n",
    "        for i, f in tqdm_bar:\n",
    "            tqdm_bar.set_description(f\"Reading {f.name}\")\n",
    "            dataset = MultiModalSeriesCSVDataset(\n",
    "                f, self.feature_prefixes, label=None, features_as_channels=True\n",
    "            )\n",
    "\n",
    "            # This should return a numpy array of shape (num_samples, num_features, num_timesteps)\n",
    "            # In this case, (num_samples, 6, 60)\n",
    "            data = dataset[:].astype(np.float32)\n",
    "            datasets.append(data)\n",
    "\n",
    "        # Data is an array of shape (num_samples, num_features, num_timesteps)\n",
    "        data = np.concatenate(datasets, axis=0)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 6 files for train: [PosixPath('/workspaces/HIAAC-KR-Dev-Container/some_datasets/DAGHAR/standardized_view/KuHar/train.csv'), PosixPath('/workspaces/HIAAC-KR-Dev-Container/some_datasets/DAGHAR/standardized_view/MotionSense/train.csv'), PosixPath('/workspaces/HIAAC-KR-Dev-Container/some_datasets/DAGHAR/standardized_view/RealWorld_thigh/train.csv'), PosixPath('/workspaces/HIAAC-KR-Dev-Container/some_datasets/DAGHAR/standardized_view/RealWorld_waist/train.csv'), PosixPath('/workspaces/HIAAC-KR-Dev-Container/some_datasets/DAGHAR/standardized_view/UCI/train.csv'), PosixPath('/workspaces/HIAAC-KR-Dev-Container/some_datasets/DAGHAR/standardized_view/WISDM/train.csv')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading train.csv: 100%|██████████| 6/6 [00:06<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape before flatten: (36788, 6, 60). Effective stride: 96 (16*6)\n",
      "Data shape after flatten (F-order): (36788, 360). Min: -66.1843032836914, Max: 72.65300750732422\n",
      "Data shape after scaling: (36788, 360). Min: -25.996206283569336, Max: 28.536834716796875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36788"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_daghar_dataset = Dataset_DAGHAR(\n",
    "    root_path=\"/workspaces/HIAAC-KR-Dev-Container/some_datasets/DAGHAR/standardized_view/\",\n",
    "    stride=stride,\n",
    "    return_values=\"x_y\",\n",
    "    flag=\"train\",\n",
    ")\n",
    "len(train_daghar_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 36788 samples. x.shape=(1, 1024), y.shape=(1, 1024)\n"
     ]
    }
   ],
   "source": [
    "index = 100\n",
    "x, y = train_daghar_dataset[index]\n",
    "print(f\"Dataset has {len(train_daghar_dataset)} samples. x.shape={x.shape}, y.shape={y.shape}\")\n",
    "\n",
    "np.testing.assert_allclose(\n",
    "    x[0, train_daghar_dataset.effective_stride:360-train_daghar_dataset.effective_stride],\n",
    "    y[0, 0:360-(train_daghar_dataset.effective_stride)*2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 6 files for val: [PosixPath('/workspaces/HIAAC-KR-Dev-Container/some_datasets/DAGHAR/standardized_view/KuHar/validation.csv'), PosixPath('/workspaces/HIAAC-KR-Dev-Container/some_datasets/DAGHAR/standardized_view/MotionSense/validation.csv'), PosixPath('/workspaces/HIAAC-KR-Dev-Container/some_datasets/DAGHAR/standardized_view/RealWorld_thigh/validation.csv'), PosixPath('/workspaces/HIAAC-KR-Dev-Container/some_datasets/DAGHAR/standardized_view/RealWorld_waist/validation.csv'), PosixPath('/workspaces/HIAAC-KR-Dev-Container/some_datasets/DAGHAR/standardized_view/UCI/validation.csv'), PosixPath('/workspaces/HIAAC-KR-Dev-Container/some_datasets/DAGHAR/standardized_view/WISDM/validation.csv')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading validation.csv: 100%|██████████| 6/6 [00:01<00:00,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape before flatten: (5844, 6, 60). Effective stride: 96 (16*6)\n",
      "Data shape after flatten (F-order): (5844, 360). Min: -50.034881591796875, Max: 52.441993713378906\n",
      "Data shape after scaling: (5844, 360). Min: -20.453800201416016, Max: 21.43722915649414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5844"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_daghar_dataset = Dataset_DAGHAR(\n",
    "    root_path=\"/workspaces/HIAAC-KR-Dev-Container/some_datasets/DAGHAR/standardized_view/\",\n",
    "    stride=stride,\n",
    "    return_values=\"x_y\",\n",
    "    flag=\"val\",\n",
    ")\n",
    "len(validation_daghar_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 6 files for test: [PosixPath('/workspaces/HIAAC-KR-Dev-Container/some_datasets/DAGHAR/standardized_view/KuHar/test.csv'), PosixPath('/workspaces/HIAAC-KR-Dev-Container/some_datasets/DAGHAR/standardized_view/MotionSense/test.csv'), PosixPath('/workspaces/HIAAC-KR-Dev-Container/some_datasets/DAGHAR/standardized_view/RealWorld_thigh/test.csv'), PosixPath('/workspaces/HIAAC-KR-Dev-Container/some_datasets/DAGHAR/standardized_view/RealWorld_waist/test.csv'), PosixPath('/workspaces/HIAAC-KR-Dev-Container/some_datasets/DAGHAR/standardized_view/UCI/test.csv'), PosixPath('/workspaces/HIAAC-KR-Dev-Container/some_datasets/DAGHAR/standardized_view/WISDM/test.csv')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading test.csv: 100%|██████████| 6/6 [00:02<00:00,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape before flatten: (9990, 6, 60). Effective stride: 96 (16*6)\n",
      "Data shape after flatten (F-order): (9990, 360). Min: -42.68782043457031, Max: 52.27903366088867\n",
      "Data shape after scaling: (9990, 360). Min: -16.971426010131836, Max: 20.78189468383789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9990"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_daghar_dataset = Dataset_DAGHAR(\n",
    "    root_path=\"/workspaces/HIAAC-KR-Dev-Container/some_datasets/DAGHAR/standardized_view/\",\n",
    "    stride=stride,\n",
    "    return_values=\"x_y\",\n",
    "    flag=\"test\",\n",
    ")\n",
    "len(test_daghar_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated train dataset has 23348457 samples\n",
      "Percentage of ES dataset: 23.43%\n"
     ]
    }
   ],
   "source": [
    "concat_es_allm4ts_train = ConcatDataset([pt_dset_train, train_es_dataset])\n",
    "print(f\"Concatenated train dataset has {len(concat_es_allm4ts_train)} samples\")\n",
    "print(f\"Percentage of ES dataset: {len(train_es_dataset)/len(concat_es_allm4ts_train)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated validation dataset has 4382103 samples\n",
      "Percentage of ES dataset: 10.26%\n"
     ]
    }
   ],
   "source": [
    "concat_es_allm4ts_val = ConcatDataset([pt_dset_val, val_es_dataset])\n",
    "print(f\"Concatenated validation dataset has {len(concat_es_allm4ts_val)} samples\")\n",
    "print(f\"Percentage of ES dataset: {len(val_es_dataset)/len(concat_es_allm4ts_val)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated test dataset has 5460178 samples\n",
      "Percentage of ES dataset: 27.98%\n"
     ]
    }
   ],
   "source": [
    "concat_es_allm4ts_test = ConcatDataset([pr_dset_test, test_es_dataset])\n",
    "print(f\"Concatenated test dataset has {len(concat_es_allm4ts_test)} samples\")\n",
    "print(f\"Percentage of ES dataset: {len(test_es_dataset)/len(concat_es_allm4ts_test)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated train dataset has 17915520 samples\n",
      "Percentage of DAGHAR data: 0.21%\n"
     ]
    }
   ],
   "source": [
    "concat_daghar_allm4ts_train = ConcatDataset([pt_dset_train, train_daghar_dataset])\n",
    "print(f\"Concatenated train dataset has {len(concat_daghar_allm4ts_train)} samples\")\n",
    "print(f\"Percentage of DAGHAR data: {len(train_daghar_dataset) / len(concat_daghar_allm4ts_train) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated validation dataset has 3938272 samples\n",
      "Percentage of DAGHAR data: 0.15%\n"
     ]
    }
   ],
   "source": [
    "concat_daghar_allm4ts_val = ConcatDataset([pt_dset_val, validation_daghar_dataset])\n",
    "print(f\"Concatenated validation dataset has {len(concat_daghar_allm4ts_val)} samples\")\n",
    "print(f\"Percentage of DAGHAR data: {len(validation_daghar_dataset) / len(concat_daghar_allm4ts_val) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated test dataset has 3942418 samples\n",
      "Percentage of DAGHAR data: 0.25%\n"
     ]
    }
   ],
   "source": [
    "concat_daghar_allm4ts_test = ConcatDataset([pr_dset_test, test_daghar_dataset])\n",
    "print(f\"Concatenated test dataset has {len(concat_daghar_allm4ts_test)} samples\")\n",
    "print(f\"Percentage of DAGHAR data: {len(test_daghar_dataset) / len(concat_daghar_allm4ts_test) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing concat_es_allm4ts_train: 100%|██████████| 23348457/23348457 [03:44<00:00, 104189.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating arrays\n",
      "Saving to /workspaces/HIAAC-KR-Dev-Container/workspace/aLLM4TS/dataset/concat_es_allm4ts_train.npz with X.shape=(23348457, 1, 1024), Y.shape=(23348457, 1, 1024)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "save_location = Path(\"/workspaces/HIAAC-KR-Dev-Container/workspace/aLLM4TS/dataset/\")\n",
    "save_location.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for dset, name in zip(\n",
    "    [concat_es_allm4ts_train, concat_es_allm4ts_val, concat_es_allm4ts_test, concat_daghar_allm4ts_train, concat_daghar_allm4ts_val, concat_daghar_allm4ts_test],\n",
    "    [\"concat_es_allm4ts_train\", \"concat_es_allm4ts_val\", \"concat_es_allm4ts_test\", \"concat_daghar_allm4ts_train\", \"concat_daghar_allm4ts_val\", \"concat_daghar_allm4ts_test\"],\n",
    "):\n",
    "    gc.collect()\n",
    "    X, Y = [], []\n",
    "    for i in tqdm.tqdm(range(len(dset)), total=len(dset), desc=f\"Processing {name}\"):\n",
    "        x, y = dset[i]\n",
    "        x = x.astype(np.float32)\n",
    "        y = y.astype(np.float32)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    print(f\"Concatenating arrays\")\n",
    "    X = np.stack(X)\n",
    "    Y = np.stack(Y)\n",
    "    print(f\"Size of X (in MB): {X.nbytes / 1024 / 1024:.2f}. Size of Y (in MB): {Y.nbytes / 1024 / 1024:.2f}\")\n",
    "    \n",
    "    fname = save_location / f\"{name}.npz\"\n",
    "    \n",
    "    print(f\"Saving to {fname} with X.shape={X.shape}, Y.shape={Y.shape}\")\n",
    "    np.savez_compressed(save_location / f\"{name}.npz\", X=X, Y=Y)\n",
    "    print(f\"Saved {name} to {save_location / f'{name}.npz'}\")\n",
    "    print()\n",
    "    \n",
    "    del X, Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
