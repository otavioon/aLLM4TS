{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from minerva.models.ssl.tfc import TFC_Model\n",
    "from minerva.models.nets.tfc import TFC_Backbone\n",
    "import warnings\n",
    "import warnings\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from minerva.data.datasets.series_dataset import MultiModalSeriesCSVDataset\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = 6\n",
    "batch_size = 128\n",
    "TS_length = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TFC_Model(\n",
       "  (backbone): TFC_Backbone(\n",
       "    (time_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=60, out_features=60, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=60, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=60, bias=True)\n",
       "          (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (frequency_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=60, out_features=60, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=60, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=60, bias=True)\n",
       "          (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (time_projector): TFC_Standard_Projector(\n",
       "      (projector): Sequential(\n",
       "        (0): Linear(in_features=360, out_features=256, bias=True)\n",
       "        (1): IgnoreWhenBatch1(\n",
       "          (module): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (frequency_projector): TFC_Standard_Projector(\n",
       "      (projector): Sequential(\n",
       "        (0): Linear(in_features=360, out_features=256, bias=True)\n",
       "        (1): IgnoreWhenBatch1(\n",
       "          (module): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (loss_fn): NTXentLoss_poly(\n",
       "    (softmax): Softmax(dim=-1)\n",
       "    (_cosine_similarity): CosineSimilarity()\n",
       "    (criterion): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TFC_Model(\n",
    "    input_channels=input_channels,\n",
    "    batch_size=batch_size,\n",
    "    TS_length=TS_length,\n",
    "    num_classes=None,\n",
    "    batch_1_correction=True,\n",
    "    backbone=TFC_Backbone(\n",
    "        input_channels=input_channels,\n",
    "        TS_length=TS_length,\n",
    "        time_encoder=TransformerEncoder(\n",
    "            TransformerEncoderLayer(\n",
    "                d_model=TS_length, dim_feedforward=2 * 128, nhead=2\n",
    "            ),\n",
    "            num_layers=2,\n",
    "        ),\n",
    "        frequency_encoder=TransformerEncoder(\n",
    "            TransformerEncoderLayer(\n",
    "                d_model=TS_length, dim_feedforward=2 * 128, nhead=2\n",
    "            ),\n",
    "            num_layers=2,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 0 has 250000 samples! (/workspaces/HIAAC-KR-Dev-Container/some_datasets/ES_Raw/es_full.160000.csv)\n",
      "Dataset 1 has 250000 samples! (/workspaces/HIAAC-KR-Dev-Container/some_datasets/ES_Raw/es_full.340000.csv)\n",
      "Dataset 2 has 160950 samples! (/workspaces/HIAAC-KR-Dev-Container/some_datasets/ES_Raw/es_full.10000.csv)\n",
      "Dataset 3 has 250000 samples! (/workspaces/HIAAC-KR-Dev-Container/some_datasets/ES_Raw/es_full.320000.csv)\n",
      "Dataset 4 has 238525 samples! (/workspaces/HIAAC-KR-Dev-Container/some_datasets/ES_Raw/es_full.300000.csv)\n",
      "Dataset 5 has 119575 samples! (/workspaces/HIAAC-KR-Dev-Container/some_datasets/ES_Raw/es_full.180000.csv)\n",
      "Dataset 6 has 250000 samples! (/workspaces/HIAAC-KR-Dev-Container/some_datasets/ES_Raw/es_full.90000.csv)\n",
      "Dataset 7 has 55925 samples! (/workspaces/HIAAC-KR-Dev-Container/some_datasets/ES_Raw/es_full.110000.csv)\n",
      "Dataset 8 has 250000 samples! (/workspaces/HIAAC-KR-Dev-Container/some_datasets/ES_Raw/es_full.200000.csv)\n",
      "Dataset 9 has 250000 samples! (/workspaces/HIAAC-KR-Dev-Container/some_datasets/ES_Raw/es_full.170000.csv)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.ConcatDataset at 0x7d3615b2d210>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = Path(\"/workspaces/HIAAC-KR-Dev-Container/some_datasets/ES_Raw/\")\n",
    "datasets = []\n",
    "\n",
    "\n",
    "for i, f in enumerate(root_dir.glob(\"es_full.*.csv\")):   \n",
    "    train_dataset = MultiModalSeriesCSVDataset(\n",
    "        f,\n",
    "        feature_prefixes=[\n",
    "            \"accel-x\",\n",
    "            \"accel-y\",\n",
    "            \"accel-z\",\n",
    "            \"gyro-x\",\n",
    "            \"gyro-y\",\n",
    "            \"gyro-z\",\n",
    "        ],\n",
    "        label=\"standard activity code\",\n",
    "        features_as_channels=True,\n",
    "    )\n",
    "    \n",
    "    datasets.append(train_dataset)\n",
    "    print(f\"Dataset {i} has {len(train_dataset)} samples! ({f})\")\n",
    "    \n",
    "train_dataset = ConcatDataset(datasets)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7d341ab3c760>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 6, 60]), torch.Size([128]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x, batch_y = next(iter(train_dataloader))\n",
    "batch_x.shape, batch_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        filename=\"tfc-{epoch:02d}\",\n",
    "        every_n_epochs=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "logger = CSVLogger(save_dir=\"./checkpoints/tfc-transformer-encoder/aLLM4TS-E/ES\", name=\"tfc-transformer-encoder\", version=\"final\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type            | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | backbone | TFC_Backbone    | 435 K  | train\n",
      "1 | loss_fn  | NTXentLoss_poly | 0      | train\n",
      "-----------------------------------------------------\n",
      "435 K     Trainable params\n",
      "0         Non-trainable params\n",
      "435 K     Total params\n",
      "1.741     Total estimated model params size (MB)\n",
      "63        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/16210 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   5%|▌         | 839/16210 [01:45<32:18,  7.93it/s, v_num=3] "
     ]
    }
   ],
   "source": [
    "# batch_x, batch_y = next(iter(train_dataloader))\n",
    "# batch_x.shape, batch_y.shape\n",
    "\n",
    "trainer.fit(\n",
    "    model, train_dataloaders=train_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
