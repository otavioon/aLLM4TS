{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch.distributed as dist\n",
    "import torch\n",
    "from exp.exp_LLM4TS import Exp_Main\n",
    "from exp.exp_imputation import Exp_Imputation\n",
    "from exp.exp_ad import Exp_Anomaly_Detection\n",
    "from exp.exp_sf import Exp_Short_Term_Forecast\n",
    "from exp.exp_classification import Exp_Classification\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "    'random_seed': 42,  # 'random seed'\n",
    "    'is_training': 1,  # 'status'\n",
    "    'model_id': 'test',  # 'model id'\n",
    "    'model': 'Autoformer',   # 'model name, options: [Autoformer, Informer, Transformer]'\n",
    "    'task_name': 'long_term_forecast',  # 'task name, options: [long_term_forecast, short_term_forecast, imputation, classification, anomaly_detection]'\n",
    "\n",
    "    # data loader\n",
    "    'data': 'ETTm1',  # 'dataset type'\n",
    "    'root_path': './data/ETT/',   # 'root path of the data file'\n",
    "    'data_path': 'ETTh1.csv',  # 'data file'\n",
    "    'features': 'M',  # 'forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate'\n",
    "    'target': 'OT',  # 'target feature in S or MS task'\n",
    "    'freq': 'h',  # 'freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h'\n",
    "    'checkpoints': './checkpoints/',  # 'location of model checkpoints'\n",
    "    'percent': 100,  \n",
    "    'mask_rate': 0.25,  # 'mask ratio'\n",
    "    'anomaly_ratio': 0.25,  # 'prior anomaly ratio (%)'\n",
    "\n",
    "    # aLLM4TS\n",
    "    'is_llm': 0,  # 'whether to use llm'\n",
    "    'pretrain': 1,  # 'whether to use pretrained llm'\n",
    "    'freeze': 1,  # 'whether to freeze specific part of the llm'\n",
    "    'llm_layers': 1,  # 'the number of llm layers we use'\n",
    "    'mask_pt': 0,  # 'mask pratrain ratio'\n",
    "    'llm': './HF_MODELS/gpt2',  # 'the llm checkpoint'\n",
    "    'attn_dropout': 0,  \n",
    "    'proj_dropout': 0,  \n",
    "    'res_attention': False,\n",
    "    \n",
    "    # SFT\n",
    "    'sft': 0,  # Wheter to use SFT\n",
    "    'sft_layers': 'null',  # 'the layers in llm needed to be trained'\n",
    "    'history_len': 0,  # 'look-back window length'\n",
    "    'fft': 0,  \n",
    "    'rand_init': 0,  # 'rand_init'\n",
    "\n",
    "    # Pretrain\n",
    "    'c_pt': 0, # 'whether continue pretrain'  \n",
    "    'pt_layers': 'null',  # 'the layers in llm needed to be trained'\n",
    "    'pt_data': 'null',  # 'the dataset used in pretrain, use _ to separate'\n",
    "    'pt_sft': 0,  # 'whether continue pretrain'\n",
    "    'pt_sft_base_dir': 'null',   # 'the base model dir for pt_sft'\n",
    "    'pt_sft_model': 'null',  # 'the base model for pt_sft'\n",
    "\n",
    "    # Forecasting task\n",
    "    'seq_len': 720,     # 'input sequence length'\n",
    "    'label_len': 0,     # 'start token length'\n",
    "    'pred_len': 720,    # 'prediction sequence length'\n",
    "    'seasonal_patterns': 'Monthly',  # 'subset for M4'\n",
    "\n",
    "    # PatchTST\n",
    "    'fc_dropout': 0.05,  # 'fully connected dropout'\n",
    "    'head_dropout': 0.0,  # 'head dropout'\n",
    "    'patch_len': 16,  # 'patch length'\n",
    "    'stride': 8,  # 'stride'\n",
    "    'padding_patch': 'end',  # 'None: None; end: padding on the end' \n",
    "    'revin': 1,  # 'RevIN; True 1 False 0'\n",
    "    'affine': 0,  # 'RevIN-affine; True 1 False 0'\n",
    "    'subtract_last': 0,   # '0: subtract mean; 1: subtract last'\n",
    "    'decomposition': 0,  # 'decomposition; True 1 False 0'\n",
    "    'kernel_size': 25,  # 'decomposition-kernel'\n",
    "    'individual': 0,  # 'individual head; True 1 False 0'\n",
    "    'notrans': False,  #  stop using transformer\n",
    "    \n",
    "    # Formers\n",
    "    'embed_type': 0,  # '0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding'\n",
    "    'enc_in': 7,  # 'encoder input size'\n",
    "    'dec_in': 7,  # 'decoder input size'\n",
    "    'c_out': 7,  # 'output size'\n",
    "    'd_model': 512,  # 'dimension of model'\n",
    "    'n_heads': 8,  # 'num of heads'\n",
    "    'e_layers': 2,  # 'num of encoder layers'\n",
    "    'd_layers': 1,  # 'num of decoder layers'\n",
    "    'd_ff': 8,  # 'dimension of fcn'\n",
    "    'moving_avg': 25,  # 'window size of moving average'\n",
    "    'factor': 1,  # 'attn factor'\n",
    "    'destill': False, # whether to use distilling in encoder, using this argument means not using distilling\n",
    "    'dropout': 0.1,  # 'dropout'\n",
    "    'embed': 'timeF',  # 'time features encoding, options:[timeF, fixed, learned]'\n",
    "    'activation': 'gelu',  # 'activation'\n",
    "    'output_attention': False,  # whether to output attention in ecoder\n",
    "    'do_predict': False,  # whether to predict unseen future data\n",
    "\n",
    "    # optimization\n",
    "    'num_workers': 4,   # 'data loader num workers'\n",
    "    'itr': 2,  # 'experiments times'\n",
    "    'train_epochs': 100,   # 'train epochs'\n",
    "    'batch_size': 128,  # 'batch size of train input data'\n",
    "    'patience': 100,  # 'early stopping patience'\n",
    "    'learning_rate': 0.0001,  # 'optimizer learning rate'\n",
    "    'des': 'test',  # 'exp description'\n",
    "    'loss': 'mse',  # 'loss function'\n",
    "    'lradj': 'type3',   # 'adjust learning rate'\n",
    "    'pct_start': 0.3,  # 'pct_start'\n",
    "    'use_amp': False,  # 'use automatic mixed precision'\n",
    "    \n",
    "    # GPU\n",
    "    'use_gpu': True,  # 'use gpu'\n",
    "    'gpu': 0,  # 'gpu'\n",
    "    'use_multi_gpu': False,  # use multiple gpus\n",
    "    'devices': '0,1,2,3',  # 'device ids of multiple gpus'\n",
    "    'test_flop': False,  # 'test model flops'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(args_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
